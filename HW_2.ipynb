{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae1d865-3ce1-485d-8d34-b0b2274c06e8",
   "metadata": {
    "id": "3ae1d865-3ce1-485d-8d34-b0b2274c06e8"
   },
   "source": [
    "# Общие указания\n",
    "\n",
    "Аналогично ДЗ 1\n",
    "\n",
    "каждое задание - 2 бала, доп задания - 1 бал. Всего 9 - потом приводится к 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034feb9c-3e42-49dd-a569-a16b8226d4be",
   "metadata": {
    "id": "034feb9c-3e42-49dd-a569-a16b8226d4be"
   },
   "source": [
    "# Задание 1.\n",
    "\n",
    "Реализуйте непараметрический LDA (лекция 2, слайд 34). Возьмите датасеты из предыдущего ДЗ (3 задание) (если не делали - подберите или сгенерируйте) и попытайтесь побить затюненный регулеризованный LDA (если не делали предыдущее задание или ваша реализация получилась неудачной, то можете взять реализацию из sklearn) подбирая kernel (перебирайте популярные, а также попробуйте придумать свой kernel), lambda (можно подбирать константу, а можно - функцию, лекция 2 - слайд 26). Сравните время работы алгоритмов.\n",
    "\n",
    "**Дополнительно**: реализуйте также local likelihood logistic regression (слайды 27 и 33 из второй лекции в помощь) и сравните с моделями из основной части."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d9e10f-00fc-4225-80e0-72bdc0eee7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81900bb5-3096-4a68-8998-c97671608e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedLDA(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, alpha=0, beta=0):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = len(self.classes_)\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        pi_k = []\n",
    "        mu_k = []\n",
    "        cov_k = []\n",
    "        for c in self.classes_:\n",
    "            X_c = X[y == c]\n",
    "            pi_k.append(len(X_c) / len(X))\n",
    "            mu_k.append(X_c.mean(axis=0))\n",
    "            cov_k.append(np.cov(X_c, rowvar=False))\n",
    "        self.pi_k = np.array(pi_k)\n",
    "        self.mu_k = np.array(mu_k)\n",
    "        self.cov_k = np.array(cov_k)\n",
    "\n",
    "        \n",
    "        sigma = np.zeros((n_features, n_features))\n",
    "        for k in range(n_classes):\n",
    "            sigma += np.sum(y == self.classes_[k]) * self.cov_k[k]\n",
    "        sigma /= (X.shape[0] - n_classes)\n",
    "        \n",
    "        diag_sigma = np.diag(np.diag(sigma))  \n",
    "        sigma = self.beta * sigma + (1 -self.beta) * diag_sigma\n",
    "\n",
    "        sigma_regularized = np.zeros_like(self.cov_k)\n",
    "        for k in range(n_classes):\n",
    "            sigma_regularized[k] = self.alpha * self.cov_k[k] + (1 - self.alpha) * sigma\n",
    "        self.sigma = sigma_regularized\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):    \n",
    "        scores = []\n",
    "        for k in range(len(self.classes_)):\n",
    "            inv_sigma_k = np.linalg.inv(self.sigma[k])\n",
    "            delta = X @ inv_sigma_k @ self.mu_k[k] - 0.5 * self.mu_k[k].T @ inv_sigma_k @ self.mu_k[k] + np.log(self.pi_k[k])\n",
    "            scores.append(delta)\n",
    "        scores = np.array(scores).T\n",
    "        return self.classes_[np.argmax(scores, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28bd03aa-7e8a-49b2-af7f-a10359c40a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonparametricLDA(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, lambda_=1, kernel='gaussian'):\n",
    "        self.lambda_ = lambda_\n",
    "        self.kernel = kernel\n",
    "        self.kernel_name_to_func = {\n",
    "            'gaussian': self.gaussian_kernel,\n",
    "            'epanechnikov': self.epanechnikov_kernel,\n",
    "            'tricube': self.tricube_kernel,\n",
    "            'test': self.test_kernel\n",
    "        }\n",
    "\n",
    "    def gaussian_kernel(self, distances, lambda_):\n",
    "        p_ = distances.shape[1]\n",
    "        return np.exp(-0.5 * (distances / lambda_)**2) / (np.sqrt(2 * np.pi) * lambda_)**(p_ / 2)\n",
    "\n",
    "    def epanechnikov_kernel(self, distances, lambda_):\n",
    "        norm_distances = distances / lambda_\n",
    "        mask = norm_distances <= 1\n",
    "        kernel_values = np.zeros_like(norm_distances)\n",
    "        kernel_values[mask] = 0.75 * (1 - norm_distances[mask]**2) / lambda_\n",
    "        return kernel_values\n",
    "\n",
    "    def tricube_kernel(self, distances, lambda_):\n",
    "        norm_distances = distances / lambda_\n",
    "        mask = norm_distances <= 1\n",
    "        kernel_values = np.zeros_like(norm_distances)\n",
    "        kernel_values[mask] = (1 - norm_distances[mask]**3)**3\n",
    "        return kernel_values / 0.874 # нормализуем чтобы использовать как плотность\n",
    "\n",
    "    def test_kernel(self, distances, lambda_):\n",
    "        return (self.tricube_kernel(distances, lambda_) + self.gaussian_kernel(distances, lambda_)) / 2\n",
    "\n",
    "    def kernel_density_estimation(self, X, data, kernel_func, lambda_):\n",
    "        distances = np.sqrt(((X[:, np.newaxis] - data)**2).sum(axis=2))\n",
    "        kernel_values = kernel_func(distances, lambda_)\n",
    "        return np.sum(kernel_values, axis=1) / len(data)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = len(self.classes_)\n",
    "        self.pi_k = np.array([np.sum(y == cls) / len(y) for cls in self.classes_])\n",
    "        self.class_data = [X[y == cls] for cls in self.classes_]\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.classes_)\n",
    "        kernel_func = self.kernel_name_to_func[self.kernel]\n",
    "        \n",
    "        density_matrix = np.zeros((n_samples, n_classes))\n",
    "        for i, cls_data in enumerate(self.class_data):\n",
    "            density_matrix[:, i] = self.kernel_density_estimation(X, cls_data, kernel_func, self.lambda_)\n",
    "        \n",
    "        class_probs = self.pi_k * density_matrix\n",
    "        norm_factors = np.sum(class_probs, axis=1, keepdims=True)\n",
    "\n",
    "        probs = class_probs / norm_factors\n",
    "\n",
    "        return self.classes_[np.argmax(probs, axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bff1b6a6-6c97-466a-b495-9ce35e771b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 'iris' with 150 objects\n",
      "RegularizedLDA time to fit: 0.0, and NonparametricLDA time to fit: 0.0009930133819580078\n",
      "RegularizedLDA time to predict: 0.0009975433349609375, and NonparametricLDA time to predict: 0.0\n",
      "RegularizedLDA best accuracy: 0.9777777777777777, with params: {'alpha': 0.0, 'beta': 0.7777777777777777}\n",
      "NonparametricLDA best accuracy: 0.9555555555555556, with params: {'kernel': 'gaussian', 'lambda_': 0.11473684210526315}\n",
      "\n",
      "Dataset: 'wine' with 178 objects\n",
      "RegularizedLDA time to fit: 0.001056671142578125, and NonparametricLDA time to fit: 0.001028299331665039\n",
      "RegularizedLDA time to predict: 0.0, and NonparametricLDA time to predict: 0.0\n",
      "RegularizedLDA best accuracy: 0.9814814814814815, with params: {'alpha': 0.0, 'beta': 0.8888888888888888}\n",
      "NonparametricLDA best accuracy: 0.9444444444444444, with params: {'kernel': 'gaussian', 'lambda_': 0.11473684210526315}\n",
      "\n",
      "Dataset: 'soybean' with 683 objects\n",
      "RegularizedLDA time to fit: 0.005000114440917969, and NonparametricLDA time to fit: 0.0026023387908935547\n",
      "RegularizedLDA time to predict: 0.002000093460083008, and NonparametricLDA time to predict: 0.019237041473388672\n",
      "RegularizedLDA best accuracy: 0.23414634146341465, with params: {'alpha': 0.0, 'beta': 0.3333333333333333}\n",
      "NonparametricLDA best accuracy: 0.9024390243902439, with params: {'kernel': 'gaussian', 'lambda_': 0.42894736842105263}\n",
      "\n",
      "Dataset: 'vehicle' with 846 objects\n",
      "RegularizedLDA time to fit: 0.0010001659393310547, and NonparametricLDA time to fit: 0.0010271072387695312\n",
      "RegularizedLDA time to predict: 0.0, and NonparametricLDA time to predict: 0.02055048942565918\n",
      "RegularizedLDA best accuracy: 0.7874015748031497, with params: {'alpha': 0.3333333333333333, 'beta': 1.0}\n",
      "NonparametricLDA best accuracy: 0.7125984251968503, with params: {'kernel': 'gaussian', 'lambda_': 0.42894736842105263}\n",
      "\n",
      "Dataset: 'tic-tac-toe' with 958 objects\n",
      "RegularizedLDA time to fit: 0.0, and NonparametricLDA time to fit: 0.0009999275207519531\n",
      "RegularizedLDA time to predict: 0.0, and NonparametricLDA time to predict: 0.01560211181640625\n",
      "RegularizedLDA best accuracy: 0.6875, with params: {'alpha': 0.1111111111111111, 'beta': 0.2222222222222222}\n",
      "NonparametricLDA best accuracy: 0.8194444444444444, with params: {'kernel': 'tricube', 'lambda_': 1.8952631578947368}\n",
      "\n",
      "Dataset: 'banknote-authentication' with 1372 objects\n",
      "RegularizedLDA time to fit: 0.000997781753540039, and NonparametricLDA time to fit: 0.0\n",
      "RegularizedLDA time to predict: 0.0, and NonparametricLDA time to predict: 0.01999974250793457\n",
      "RegularizedLDA best accuracy: 0.9854368932038835, with params: {'alpha': 1.0, 'beta': 0.0}\n",
      "NonparametricLDA best accuracy: 1.0, with params: {'kernel': 'tricube', 'lambda_': 0.7431578947368421}\n",
      "\n",
      "Dataset: 'spambase' with 4601 objects\n",
      "RegularizedLDA time to fit: 0.0020034313201904297, and NonparametricLDA time to fit: 0.0010211467742919922\n",
      "RegularizedLDA time to predict: 0.0009932518005371094, and NonparametricLDA time to predict: 2.045428514480591\n",
      "RegularizedLDA best accuracy: 0.9333816075307748, with params: {'alpha': 0.7777777777777777, 'beta': 0.4444444444444444}\n",
      "NonparametricLDA best accuracy: 0.8139029688631426, with params: {'kernel': 'tricube', 'lambda_': 2.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "dataset_names = [\"iris\", \"wine\", \"soybean\", \"vehicle\", \"tic-tac-toe\", \"banknote-authentication\", \"spambase\"]\n",
    "\n",
    "datasets = []\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "for name in dataset_names:\n",
    "    data = fetch_openml(name, as_frame=True, parser=\"pandas\")\n",
    "    X = data.data\n",
    "    y = data.target.astype(\"category\").cat.codes\n",
    "    datasets.append((name, X, y))\n",
    "\n",
    "\n",
    "lda_reg_params = {'alpha': np.linspace(0, 1, 10), 'beta': np.linspace(0, 1, 10)}\n",
    "lda_np_params = {'lambda_': np.linspace(0.01, 2, 20), 'kernel': ['gaussian', 'epanechnikov', 'tricube', 'test']}\n",
    "\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for name, X, y in datasets:\n",
    "    \n",
    "    num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    cat_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    for col in cat_cols:\n",
    "         X[col] = LabelEncoder().fit_transform(X[col])\n",
    "    \n",
    "    if y.dtype == 'object':\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "    else:\n",
    "        y = y.astype(int)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=424, stratify=y)\n",
    "\n",
    "    \n",
    "    lda_reg = RegularizedLDA()\n",
    "    lda_reg_gs = GridSearchCV(lda_reg, param_grid=lda_reg_params, cv=StratifiedKFold(5), scoring=\"accuracy\")\n",
    "    lda_reg_gs.fit(X_train, y_train)\n",
    "\n",
    "    start_reg_fit = time.time()\n",
    "    RegularizedLDA().fit(X_train, y_train)\n",
    "    elapsed_reg_time_fit = time.time() - start_reg_fit\n",
    "    \n",
    "    lda_reg_best = lda_reg_gs.best_estimator_\n",
    "\n",
    "    start_reg = time.time()\n",
    "    lda_reg_pred = lda_reg_best.predict(X_test)\n",
    "    elapsed_reg_time = time.time() - start_reg\n",
    "\n",
    "    lda_reg_acc = accuracy_score(y_test, lda_reg_pred)\n",
    "\n",
    "    lda_np = NonparametricLDA()\n",
    "    lda_np_gs = GridSearchCV(lda_np, param_grid=lda_np_params, cv=StratifiedKFold(5), scoring=\"accuracy\")\n",
    "    lda_np_gs.fit(X_train, y_train)\n",
    "    start_np_fit = time.time()\n",
    "    NonparametricLDA().fit(X_train, y_train)\n",
    "    elapsed_np_time_fit = time.time() - start_np_fit\n",
    "\n",
    "    lda_np_best = lda_np_gs.best_estimator_\n",
    "    \n",
    "    start_np = time.time()\n",
    "    lda_np_pred = lda_np_best.predict(X_test)\n",
    "    elapsed_np_time = time.time() - start_np\n",
    "\n",
    "    lda_np_acc = accuracy_score(y_test, lda_np_pred)\n",
    "\n",
    "    best_models[name] = {'lda_reg_best_params': lda_reg_gs.best_params_, 'lda_np_best_params': lda_np_gs.best_params_}\n",
    "    \n",
    "    print(f\"Dataset: '{name}' with {len(y)} objects\")\n",
    "    print(f\"RegularizedLDA time to fit: {elapsed_reg_time_fit}, and NonparametricLDA time to fit: {elapsed_np_time_fit}\")\n",
    "    print(f\"RegularizedLDA time to predict: {elapsed_reg_time}, and NonparametricLDA time to predict: {elapsed_np_time}\")\n",
    "    print(f\"RegularizedLDA best accuracy: {lda_reg_acc}, with params: {lda_reg_gs.best_params_}\")\n",
    "    print(f\"NonparametricLDA best accuracy: {lda_np_acc}, with params: {lda_np_gs.best_params_}\", end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d8a60-aa96-4d19-a335-ddaa8098e2b5",
   "metadata": {},
   "source": [
    "Получилось побить регуляризованный LDA на некоторых датасетах, причем в одном случае очень значительно (0.23 против 0.90). По поводу времени работы можно увидеть, что предсказание у непараметрического LDA занимает очень значительное время, которое сильно зависит от размера датасета."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96639f52-d61d-48df-957c-13660eeca621",
   "metadata": {},
   "source": [
    "#### Доп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "856a97fc-d2a3-4d5f-9a81-6494dc3057eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class LocalLogisticRegression(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, lambda_=1, kernel='gaussian', max_iter=100):\n",
    "        self.lambda_ = lambda_\n",
    "        self.kernel = kernel\n",
    "        self.max_iter = max_iter\n",
    "        self.kernel_name_to_func = {\n",
    "            'gaussian': self.gaussian_kernel,\n",
    "            'epanechnikov': self.epanechnikov_kernel,\n",
    "            'tricube': self.tricube_kernel,\n",
    "            'test': self.test_kernel\n",
    "        }\n",
    "\n",
    "    def gaussian_kernel(self, distances, lambda_):\n",
    "        return np.exp(-0.5 * (distances / lambda_)**2)\n",
    "\n",
    "    def epanechnikov_kernel(self, distances, lambda_):\n",
    "        norm_distances = distances / lambda_\n",
    "        mask = norm_distances <= 1\n",
    "        kernel_values = np.zeros_like(norm_distances)\n",
    "        kernel_values[mask] = 0.75 * (1 - norm_distances[mask]**2)\n",
    "        return kernel_values\n",
    "\n",
    "    def tricube_kernel(self, distances, lambda_):\n",
    "        norm_distances = distances / lambda_\n",
    "        mask = norm_distances <= 1\n",
    "        kernel_values = np.zeros_like(norm_distances)\n",
    "        kernel_values[mask] = (1 - norm_distances[mask]**3)**3\n",
    "        return kernel_values\n",
    "\n",
    "    def test_kernel(self, distances, lambda_):\n",
    "        return (self.tricube_kernel(distances, lambda_) + self.gaussian_kernel(distances, lambda_)) / 2\n",
    "\n",
    "    def kernel_weights(self, X_train, X_test):\n",
    "        distances = np.sqrt(((X_test[:, np.newaxis, :] - X_train[np.newaxis, :, :])**2).sum(axis=2))\n",
    "        kernel_func = self.kernel_name_to_func[self.kernel]\n",
    "        return kernel_func(distances, self.lambda_)\n",
    "\n",
    "    def _softmax(self, logits):\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        return exp_logits / exp_logits.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def _log_likelihood(self, params, X, y, kernel_weights, num_classes):\n",
    "        n_samples, n_features = X.shape\n",
    "        intercepts = params[:num_classes]\n",
    "        coefs = params[num_classes:].reshape(num_classes, n_features)\n",
    "        logits = intercepts + X @ coefs.T\n",
    "        probs = self._softmax(logits)\n",
    "        ll = np.sum(kernel_weights * np.log(probs[np.arange(n_samples), y]))\n",
    "        return -ll\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_ = np.asarray(X)\n",
    "        self.y_ = np.asarray(y)\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.num_classes_ = len(self.classes_)\n",
    "        self.n_features_ = X.shape[1]\n",
    "        return self\n",
    "\n",
    "    def _global_fit(self, kernel_weights):\n",
    "        initial_params = np.zeros(self.num_classes_ + self.num_classes_ * self.n_features_)\n",
    "        result = minimize(\n",
    "            self._log_likelihood,\n",
    "            initial_params,\n",
    "            args=(self.X_, self.y_, kernel_weights, self.num_classes_),\n",
    "            method='L-BFGS-B',\n",
    "            options={'maxiter': self.max_iter}\n",
    "        )\n",
    "        params = result.x\n",
    "        intercepts = params[:self.num_classes_]\n",
    "        coefs = params[self.num_classes_:].reshape(self.num_classes_, self.n_features_)\n",
    "        return intercepts, coefs\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X)\n",
    "        kernel_weights = self.kernel_weights(self.X_, X)\n",
    "        intercepts, coefs = self._global_fit(kernel_weights)\n",
    "        logits = intercepts + X @ coefs.T\n",
    "        return self._softmax(logits)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(probas, axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b50e6ef7-d4c0-4a2e-93b1-450ec52d92b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 'iris' with 150 objects\n",
      "LocalLogisticRegression time to fit: 0.0\n",
      "LocalLogisticRegression time to predict: 0.04315900802612305\n",
      "LocalLogisticRegression best accuracy: 0.9555555555555556, with params: {'kernel': 'gaussian', 'lambda_': 0.23111111111111113}\n",
      "\n",
      "Dataset: 'wine' with 178 objects\n",
      "LocalLogisticRegression time to fit: 0.0\n",
      "LocalLogisticRegression time to predict: 0.09591817855834961\n",
      "LocalLogisticRegression best accuracy: 0.9629629629629629, with params: {'kernel': 'gaussian', 'lambda_': 1.557777777777778}\n",
      "\n",
      "Dataset: 'soybean' with 683 objects\n",
      "LocalLogisticRegression time to fit: 0.0\n",
      "LocalLogisticRegression time to predict: 6.011977434158325\n",
      "LocalLogisticRegression best accuracy: 0.9170731707317074, with params: {'kernel': 'gaussian', 'lambda_': 2.0}\n",
      "\n",
      "Dataset: 'vehicle' with 846 objects\n",
      "LocalLogisticRegression time to fit: 0.0\n",
      "LocalLogisticRegression time to predict: 6.543075799942017\n",
      "LocalLogisticRegression best accuracy: 0.7677165354330708, with params: {'kernel': 'gaussian', 'lambda_': 1.3366666666666667}\n",
      "\n",
      "Dataset: 'tic-tac-toe' with 958 objects\n",
      "LocalLogisticRegression time to fit: 0.0\n",
      "LocalLogisticRegression time to predict: 0.09953498840332031\n",
      "LocalLogisticRegression best accuracy: 0.6493055555555556, with params: {'kernel': 'tricube', 'lambda_': 1.778888888888889}\n",
      "\n",
      "Dataset: 'banknote-authentication' with 1372 objects\n",
      "LocalLogisticRegression time to fit: 0.0\n",
      "LocalLogisticRegression time to predict: 0.7583062648773193\n",
      "LocalLogisticRegression best accuracy: 0.9927184466019418, with params: {'kernel': 'epanechnikov', 'lambda_': 0.23111111111111113}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "dataset_names = [\"iris\", \"wine\", \"soybean\", \"vehicle\", \"tic-tac-toe\", \"banknote-authentication\"]\n",
    "\n",
    "datasets = []\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "for name in dataset_names:\n",
    "    data = fetch_openml(name, as_frame=True, parser=\"pandas\")\n",
    "    X = data.data\n",
    "    y = data.target.astype(\"category\").cat.codes\n",
    "    datasets.append((name, X, y))\n",
    "\n",
    "llr_params = {'lambda_': np.linspace(0.01, 2, 10), 'kernel': ['gaussian', 'epanechnikov', 'tricube']}\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for name, X, y in datasets:\n",
    "    \n",
    "    num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    cat_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    for col in cat_cols:\n",
    "         X[col] = LabelEncoder().fit_transform(X[col])\n",
    "    \n",
    "    if y.dtype == 'object':\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "    else:\n",
    "        y = y.astype(int)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=424, stratify=y)\n",
    "\n",
    "    llr = LocalLogisticRegression()\n",
    "    llr_gs = GridSearchCV(llr, param_grid=llr_params, cv=StratifiedKFold(5), scoring=\"accuracy\")\n",
    "    llr_gs.fit(X_train, y_train)\n",
    "    llr_fit = time.time()\n",
    "    LocalLogisticRegression().fit(X_train, y_train)\n",
    "    elapsed_llr_time_fit = time.time() - llr_fit\n",
    "\n",
    "    llr_best = llr_gs.best_estimator_\n",
    "    \n",
    "    start_llr = time.time()\n",
    "    llr_pred = llr_best.predict(X_test)\n",
    "    elapsed_llr_time = time.time() - start_llr\n",
    "\n",
    "    llr_acc = accuracy_score(y_test, llr_pred)\n",
    "\n",
    "    best_models[name] = {'lda_np_best_params': llr_gs.best_params_}\n",
    "    \n",
    "    print(f\"Dataset: '{name}' with {len(y)} objects\")\n",
    "    print(f\"LocalLogisticRegression time to fit: {elapsed_llr_time_fit}\")\n",
    "    print(f\"LocalLogisticRegression time to predict: {elapsed_llr_time}\")\n",
    "    print(f\"LocalLogisticRegression best accuracy: {llr_acc}, with params: {llr_gs.best_params_}\", end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1074fc0b-674f-4df9-b8c8-e683b5f51a42",
   "metadata": {},
   "source": [
    "LocalLogisticRegression  получилось побить оба LDA метода только на одном датасете, однако оно работает значительно дольше даже параметрического LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615cb4e4-1b91-479b-a894-9f2e48ded066",
   "metadata": {
    "id": "615cb4e4-1b91-479b-a894-9f2e48ded066"
   },
   "source": [
    "# Задание 2.\n",
    "\n",
    "Используйте kernel trick для классификации графов с помощью ridge регрессии (она используется для регрессии, но здесь мы сделаем вид, что все ок и будем обрубать значения меньше 0 и больше 1) и svm (в Sklearn можно использовать кастомные kernels).\n",
    "\n",
    "Датасеты отсюда: https://github.com/FilippoMB/Benchmark_dataset_for_graph_classification\n",
    "\n",
    "Ядра можно взять отсюда: https://github.com/ysig/GraKeL\n",
    "\n",
    "Подберите лучшее ядро для всех случаев.\n",
    "\n",
    "**Дополнительно**: поэксперементируйте с kernel construction на основе kernels из библиотеки и попробуйте предложить свой kernel, который работает лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3453341e-1677-4128-8749-0c9870b8202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from grakel import datasets, GraphKernel, graph_from_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ce710fd-5b92-460b-91aa-004b753a7e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_names = [\n",
    "    \"shortest_path\", \n",
    "    \"graphlet_sampling\", \n",
    "    \"pyramid_match\", \n",
    "    \"svm_theta\",\n",
    "    \"neighborhood_hash\",\n",
    "    \"subtree_wl\",\n",
    "    \"odd_sth\",\n",
    "    \"propagation\",\n",
    "    \"vertex_histogram\",\n",
    "    \"weisfeiler_lehman\",\n",
    "    \"core_framework\"\n",
    "]\n",
    "\n",
    "def ridge_clf(K_train, y_train, K_test):\n",
    "    model = KernelRidge(alpha=1.0, kernel='precomputed')\n",
    "    model.fit(K_train, y_train)\n",
    "    y_pred = model.predict(K_test)\n",
    "    return np.clip(np.round(y_pred), 0, 1).astype(int)\n",
    "\n",
    "# graph_dataset_names = [\"easy_small\", \"easy\", \"hard_small\", \"hard\"]\n",
    "graph_dataset_names = [\"hard_small\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daa241e0-94f1-4bd2-b1e8-16190d04d4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "dataset:  hard_small\n",
      "--------------------------------\n",
      "shortest_path -- SVM Accuracy: 69.23% | Ridge Accuracy: 42.31% | Time: 2.78s\n",
      "graphlet_sampling -- SVM Accuracy: 38.46% | Ridge Accuracy: 34.62% | Time: 31.61s\n",
      "pyramid_match -- SVM Accuracy: 23.08% | Ridge Accuracy: 34.62% | Time: 2.19s\n",
      "svm_theta -- SVM Accuracy: 15.38% | Ridge Accuracy: 50.0% | Time: 0.68s\n",
      "neighborhood_hash -- SVM Accuracy: 61.54% | Ridge Accuracy: 38.46% | Time: 2.12s\n",
      "subtree_wl -- SVM Accuracy: 15.38% | Ridge Accuracy: 34.62% | Time: 0.01s\n",
      "odd_sth -- SVM Accuracy: 42.31% | Ridge Accuracy: 34.62% | Time: 14.29s\n",
      "propagation -- SVM Accuracy: 53.85% | Ridge Accuracy: 34.62% | Time: 1.64s\n",
      "vertex_histogram -- SVM Accuracy: 15.38% | Ridge Accuracy: 34.62% | Time: 0.01s\n",
      "weisfeiler_lehman -- SVM Accuracy: 23.08% | Ridge Accuracy: 34.62% | Time: 0.37s\n",
      "core_framework -- SVM Accuracy: 69.23% | Ridge Accuracy: 42.31% | Time: 13.15s\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in graph_dataset_names:\n",
    "    \n",
    "    print(\"--------------------------------\")\n",
    "    print(\"dataset: \", dataset_name)\n",
    "    print(\"--------------------------------\")\n",
    "    \n",
    "    loaded = np.load(f'datasets/{dataset_name}.npz', allow_pickle=True)\n",
    "    A_train = list(loaded['tr_adj'])\n",
    "    X_train = loaded['tr_feat']\n",
    "    y_train = np.argmax(loaded['tr_class'], axis=-1)\n",
    "    A_test = list(loaded['te_adj'])\n",
    "    X_test = loaded['te_feat']\n",
    "    y_test = np.argmax(loaded['te_class'], axis=-1)\n",
    "\n",
    "    X_train = np.nan_to_num(X_train)\n",
    "    X_test = np.nan_to_num(X_test)\n",
    "    \n",
    "    G_tr, G_te = [], []\n",
    "    for a, x in zip(A_train, X_train):\n",
    "        G = nx.from_scipy_sparse_array(a)\n",
    "        nx.set_node_attributes(G, dict(enumerate(map(tuple, x))), 'features')\n",
    "        G_tr.append(G)\n",
    "    for a, x in zip(A_test, X_test):\n",
    "        G = nx.from_scipy_sparse_array(a)\n",
    "        nx.set_node_attributes(G, dict(enumerate(map(tuple, x))), 'features')\n",
    "        G_te.append(G)\n",
    "    \n",
    "    G_train = [g for g in graph_from_networkx(G_tr, node_labels_tag='features')]\n",
    "    G_test = [g for g in graph_from_networkx(G_te, node_labels_tag='features')]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for k_ in kernel_names:\n",
    "        start = time()\n",
    "    \n",
    "        gk = GraphKernel(kernel=[{\"name\": k_}], normalize=True)\n",
    "\n",
    "        K_train = np.nan_to_num(gk.fit_transform(G_train))\n",
    "        K_test = np.nan_to_num(gk.transform(G_test))\n",
    "        \n",
    "        svm_clf = svm.SVC(kernel='precomputed', C=1)\n",
    "        svm_clf.fit(K_train, y_train)\n",
    "        svm_y_pred = svm_clf.predict(K_test)\n",
    "        svm_acc = accuracy_score(y_test, svm_y_pred)\n",
    "    \n",
    "        ridge_y_pred = ridge_clf(K_train, y_train, K_test)\n",
    "        ridge_acc = accuracy_score(y_test, ridge_y_pred)\n",
    "    \n",
    "        end = time()\n",
    "        print(f\"{k_} -- SVM Accuracy: {round(svm_acc*100, 2)}% | Ridge Accuracy: {round(ridge_acc*100, 2)}% | Time: {round(end - start, 2)}s\")\n",
    "\n",
    "    print(\"--------------------------------\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e1890-54de-4fae-b2e9-e459ef40cf38",
   "metadata": {},
   "source": [
    "Видим что лучшие ядра -- это core_framework и shortest_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb9b1b-c9ce-448b-85b2-b8de71ef2393",
   "metadata": {},
   "source": [
    "#### Доп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "728b14e6-7751-4f5a-bd75-aa174f3f5c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grakel import Kernel\n",
    "\n",
    "class ScaledKernel(Kernel):\n",
    "    def __init__(self, base_kernel, scale=1.0):\n",
    "        self.base_kernel = base_kernel\n",
    "        self.scale = scale\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        K = self.base_kernel.fit_transform(X)\n",
    "        return self.scale * K\n",
    "\n",
    "    def transform(self, X):\n",
    "        K = self.base_kernel.transform(X)\n",
    "        return self.scale * K\n",
    "\n",
    "\n",
    "class ExpKernel(Kernel):\n",
    "    def __init__(self, base_kernel):\n",
    "        self.base_kernel = base_kernel\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        K = self.base_kernel.fit_transform(X)\n",
    "        return np.exp(K)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        K = self.base_kernel.transform(X)\n",
    "        return np.exp(K)\n",
    "\n",
    "\n",
    "class SumKernel(Kernel):\n",
    "    def __init__(self, base_kernel1, base_kernel2):\n",
    "        self.base_kernel1 = base_kernel1\n",
    "        self.base_kernel2 = base_kernel2\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        K1 = self.base_kernel1.fit_transform(X)\n",
    "        K2 = self.base_kernel2.fit_transform(X)\n",
    "        return K1 + K2\n",
    "        \n",
    "    def transform(self, X):\n",
    "        K1 = self.base_kernel1.transform(X)\n",
    "        K2 = self.base_kernel2.transform(X)\n",
    "        return K1 + K2\n",
    "\n",
    "my_kernels = [ExpKernel(GraphKernel(kernel=[{\"name\": \"shortest_path\"}], normalize=True)),\n",
    "              ScaledKernel(ExpKernel(GraphKernel(kernel=[{\"name\": \"core_framework\"}], normalize=True)), scale = 4.0),\n",
    "              SumKernel(GraphKernel(kernel=[{\"name\": \"shortest_path\"}], normalize=True), ExpKernel(GraphKernel(kernel=[{\"name\": \"weisfeiler_lehman\"}], normalize=True)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b8161a8-293f-4560-9d2c-72959d83927a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "dataset:  hard_small\n",
      "--------------------------------\n",
      "ExpKernel(base_kernel=GraphKernel(kernel=[{'name': 'shortest_path'}],\n",
      "                                  normalize=True)) -- SVM Accuracy: 73.08% | Ridge Accuracy: 46.15% | Time: 2.72s\n",
      "ScaledKernel(base_kernel=ExpKernel(base_kernel=GraphKernel(kernel=[{'name': 'core_framework'}],\n",
      "                                                           normalize=True)),\n",
      "             scale=4.0) -- SVM Accuracy: 76.92% | Ridge Accuracy: 46.15% | Time: 13.18s\n",
      "SumKernel(base_kernel1=GraphKernel(kernel=[{'name': 'shortest_path'}],\n",
      "                                   normalize=True),\n",
      "          base_kernel2=ExpKernel(base_kernel=GraphKernel(kernel=[{'name': 'weisfeiler_lehman'}],\n",
      "                                                         normalize=True))) -- SVM Accuracy: 65.38% | Ridge Accuracy: 42.31% | Time: 3.1s\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in graph_dataset_names:\n",
    "    \n",
    "    print(\"--------------------------------\")\n",
    "    print(\"dataset: \", dataset_name)\n",
    "    print(\"--------------------------------\")\n",
    "    \n",
    "    loaded = np.load(f'datasets/{dataset_name}.npz', allow_pickle=True)\n",
    "    A_train = list(loaded['tr_adj'])\n",
    "    X_train = loaded['tr_feat']\n",
    "    y_train = np.argmax(loaded['tr_class'], axis=-1)\n",
    "    A_test = list(loaded['te_adj'])\n",
    "    X_test = loaded['te_feat']\n",
    "    y_test = np.argmax(loaded['te_class'], axis=-1)\n",
    "\n",
    "    X_train = np.nan_to_num(X_train)\n",
    "    X_test = np.nan_to_num(X_test)\n",
    "    \n",
    "    G_tr, G_te = [], []\n",
    "    for a, x in zip(A_train, X_train):\n",
    "        G = nx.from_scipy_sparse_array(a)\n",
    "        nx.set_node_attributes(G, dict(enumerate(map(tuple, x))), 'features')\n",
    "        G_tr.append(G)\n",
    "    for a, x in zip(A_test, X_test):\n",
    "        G = nx.from_scipy_sparse_array(a)\n",
    "        nx.set_node_attributes(G, dict(enumerate(map(tuple, x))), 'features')\n",
    "        G_te.append(G)\n",
    "    \n",
    "    G_train = [g for g in graph_from_networkx(G_tr, node_labels_tag='features')]\n",
    "    G_test = [g for g in graph_from_networkx(G_te, node_labels_tag='features')]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for gk in my_kernels:\n",
    "        start = time()\n",
    "        K_train = np.nan_to_num(gk.fit_transform(G_train))\n",
    "        K_test = np.nan_to_num(gk.transform(G_test))\n",
    "        \n",
    "        svm_clf = svm.SVC(kernel='precomputed', C=1)\n",
    "        svm_clf.fit(K_train, y_train)\n",
    "        svm_y_pred = svm_clf.predict(K_test)\n",
    "        svm_acc = accuracy_score(y_test, svm_y_pred)\n",
    "    \n",
    "        ridge_y_pred = ridge_clf(K_train, y_train, K_test)\n",
    "        ridge_acc = accuracy_score(y_test, ridge_y_pred)\n",
    "    \n",
    "        end = time()\n",
    "        print(f\"{gk} -- SVM Accuracy: {round(svm_acc*100, 2)}% | Ridge Accuracy: {round(ridge_acc*100, 2)}% | Time: {round(end - start, 2)}s\")\n",
    "\n",
    "    print(\"--------------------------------\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779414dc-4fb8-49a8-bb68-2b2d6cef4247",
   "metadata": {},
   "source": [
    "Получилось побить базовые ядра с помощью 4 * экспоненту от core_framework (SVM Accuracy: 76.92%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ac0a0-4451-4ce3-a280-da2beae5a358",
   "metadata": {
    "id": "107ac0a0-4451-4ce3-a280-da2beae5a358"
   },
   "source": [
    "# Задание 3.\n",
    "\n",
    "Подберите несколько датасетов с большим количеством предикторов (>20) из UCI репозитория или любого другого (классификация и/или регрессия)). Реализуйте gaussian process (можно взять готовый но скорее всего его все равно придется модфицировать) для подбора гиперпараметров random forest (или какого либо бустинга) (смотрите лекцию 3). Сравните скорость с random search (попытайтесь презойти).\n",
    "\n",
    "**Дополнительно**: поэксперементируйте с критериями подбора следующей точки (лекция 3) и ядрами, а также с самими критериями качеста (есть ли отличие в эффективности gaussian process для разных критериев)\n",
    "\n",
    "P. S.: в этом задании оцениваться будет прежде всего правильность реализации, а не скорость, т.к. сделать быстрее рандома (в виду требуемых доп вычислений) может быть сложно и это зависит от датасета, но нужно попытаться реализовать как можно эффективнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e739bb59-b35e-4b92-953d-219635ed07c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Categorical\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d7b03e2-b7c3-4dfe-ba88-b7e75d57b0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: spambase\n",
      "Method              Best Score     Time (s)  \n",
      "---------------------------------------------\n",
      "BayesSearchCV       0.9520         79.07     \n",
      "RandomizedSearchCV  0.9446         16.82     \n",
      "\n",
      "Processing dataset: steel-plates-fault\n",
      "Method              Best Score     Time (s)  \n",
      "---------------------------------------------\n",
      "BayesSearchCV       0.9964         73.16     \n",
      "RandomizedSearchCV  0.9948         13.63     \n",
      "\n",
      "Processing dataset: one-hundred-plants-shape\n",
      "Method              Best Score     Time (s)  \n",
      "---------------------------------------------\n",
      "BayesSearchCV       0.6400         201.40    \n",
      "RandomizedSearchCV  0.6138         68.73     \n",
      "\n",
      "Processing dataset: scene\n",
      "Method              Best Score     Time (s)  \n",
      "---------------------------------------------\n",
      "BayesSearchCV       0.9244         240.19    \n",
      "RandomizedSearchCV  0.9206         68.96     \n"
     ]
    }
   ],
   "source": [
    "def execute_optimization(features, labels, method_name, optimizer):\n",
    "    start_time = time()\n",
    "    optimizer.fit(features, labels)\n",
    "    duration = time() - start_time\n",
    "    best_score = optimizer.best_score_\n",
    "    return {\"best_score\": best_score, \"time\": duration}\n",
    "\n",
    "def bayes_optimization(features, labels, model, cv_splitter, iterations):\n",
    "    bayes_optimizer = BayesSearchCV(\n",
    "        estimator=model,\n",
    "        search_spaces={\n",
    "            \"n_estimators\": Integer(10, 200),\n",
    "            \"max_depth\": Integer(1, 20),\n",
    "            \"min_samples_split\": Integer(2, 20),\n",
    "            \"min_samples_leaf\": Integer(1, 20),\n",
    "            \"bootstrap\": Categorical([True, False]),\n",
    "        },\n",
    "        n_iter=iterations,\n",
    "        n_jobs=-1,\n",
    "        cv=cv_splitter,\n",
    "        return_train_score=False,\n",
    "        optimizer_kwargs={\"base_estimator\": \"GP\"},\n",
    "    )\n",
    "    return execute_optimization(features, labels, \"BayesSearchCV (GP)\", bayes_optimizer)\n",
    "\n",
    "def random_search_optimization(features, labels, model, cv_splitter, iterations):\n",
    "    param_distribution = {\n",
    "        \"n_estimators\": randint(10, 200),\n",
    "        \"max_depth\": randint(1, 20),\n",
    "        \"min_samples_split\": randint(2, 20),\n",
    "        \"min_samples_leaf\": randint(1, 20),\n",
    "        \"bootstrap\": [True, False],\n",
    "    }\n",
    "\n",
    "    random_optimizer = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_distribution,\n",
    "        n_iter=iterations,\n",
    "        n_jobs=-1,\n",
    "        cv=cv_splitter,\n",
    "        return_train_score=False,\n",
    "        scoring=\"accuracy\",\n",
    "    )\n",
    "    return execute_optimization(features, labels, \"RandomizedSearchCV\", random_optimizer)\n",
    "\n",
    "\n",
    "\n",
    "dataset_collection = {}\n",
    "dataset_list = [\"spambase\", \"steel-plates-fault\", \"one-hundred-plants-shape\", \"scene\"]\n",
    "\n",
    "for dataset in dataset_list:\n",
    "    data = fetch_openml(name=dataset, as_frame=False)\n",
    "    features, labels = data.data, data.target.astype(int)\n",
    "    dataset_collection[dataset] = (features, labels)\n",
    "\n",
    "cv_splitter = StratifiedKFold(n_splits=5, random_state=22, shuffle=True)\n",
    "base_model = RandomForestClassifier(random_state=42)\n",
    "iteration_count = 40\n",
    "\n",
    "results = {}\n",
    "for dataset_name, (features, labels) in dataset_collection.items():\n",
    "    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "    results[dataset_name] = {}\n",
    "    results[dataset_name][\"BayesSearchCV\"] = bayes_optimization(features, labels, base_model, cv_splitter, iteration_count)\n",
    "    results[dataset_name][\"RandomizedSearchCV\"] = random_search_optimization(features, labels, base_model, cv_splitter, iteration_count)\n",
    "\n",
    "    print(f\"{'Method':<20}{'Best Score':<15}{'Time (s)':<10}\")\n",
    "    print(\"-\" * 45)\n",
    "    for method_name, metrics in results[dataset_name].items():\n",
    "        print(f\"{method_name:<20}{metrics['best_score']:<15.4f}{metrics['time']:<10.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745755a8-29eb-43c8-adb3-02d1dd32f83a",
   "metadata": {},
   "source": [
    "Получили результаты с помощью gaussian process немного лучше чем через random search, но затраченное время сильно выше\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
